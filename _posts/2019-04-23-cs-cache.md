---
layout: post
title: Cache
category:
- Computer Science series
---

## Intro

Caching means saving and reusing the result for specific request without reprocessing repeatedly.

Cache entry is created/saved in key-value map: input <-> output

Cache entry can be expired or invalid: When system no longer produces the same output as cached input, the cache is outdated

## Reading: hit or miss

Each time a client requests to the cache, the cache first check if the entry exist by search by key:

- If hit: the cache return value
- If miss:
  - The cache read data from underlying store
  - Save that data into cache: key-value
  - Return the data to client

## Writing/Creating policies

For the writing operation, when writing data to cache, it must write that data to the backing store as well:
[Wikipedia writing policies](https://en.wikipedia.org/wiki/Cache_(computing)#Writing_policies)

### Write-through

Write is done synchronously both to the cache and to the backing store

- Read operation:
  - On read-hit: just return cache entry value
  - On read-miss:
    - Read data from store
    - Find possible slot (empty or least recent used)
    - Write to cache entry
    - Return cache entry value
- Write operation:
  - On write-hit: overwrite to cache + write to store
  - On write-miss: write to store

### Write-back (write-behind)

Some concepts:

- A cache entry is "dirty" if its value isn't written to store yet.
- Lazy-write: we write a cached value to the store, only when that cached value is about to be overwritten.
- In a write operation, if write-miss, we have 2 options:
  - Create a cache entry: called "write allocate" (aka fetch on write)
  - Don't create a cache entry: called "no-write allocate" (aka write-no-allocate or write around)
- Risk to lost data: when cache is shutdown ungracefully, dirty data isn't written to store

For the case cache miss, when creating a cache entry:

- If there is an empty slot: use that slot
- If no empty slot:
  - Find a possible existing entry to replace (many strategies)
  - If the existing entry is dirty, write its value to store
  - Mark the entry as "not-dirty" (since the data is the same in cache and store)
- Set the key-value to that entry

#### Read operation

- On read-hit: just return cache entry value
- On read-miss:
  - Read data from store, and create a cache entry as above strategy
  - Return cache entry value

#### Write operation

- On write-hit: overwrite to cache entry
- On write-miss:
  - For strategy "write allocate" (update the cache on write)
    - Create a cache entry as above strategy
  - For strategy "no-write allocate" (don't update the cache on write)
    - Just write to store
    - Don't create cache entry
- Mark cache entry as dirty (means not written to store yet)

### Write-through vs write-back

- Write-through: data consistency bw cache and store
- Write-back:
  - Store is only updated when a cache entry is overwritten
  - Dirty entries can be lost if cache is shutdown unexpectedly
  - Extra work to track if a cache entry is dirty or not
- In read-miss:
  - Write-through: 1 operations: Load from store
  - Write-back: 2 operations: Write to store + Load from store
- For write-through, we always need to write directly to store, creating cache on write-miss doesn't benefit.
So, no-write-allocate strategy is preferred.
- For write-back, we don't always write to store, instead to cache, so creating cache on write-miss benefits
on next write-hit.

## Cache coherence

Here we will make sure that cached value is up-to-date, because there is cases:


[For multiple caches](https://en.wikipedia.org/wiki/Cache_coherence)

## Replacement policies

 making heavy time-consuming task into lightweight less time-consuming task, examples:

- Database querying, with input is the query, output is result:
  - Without caching: heavy querying for each request
  - Caching: lookup for saved result for each request (lighter)
- CDN, with input is the request, output is the resource:
  - Without caching: request-response go through multiple network hops
  - With caching (nearby cached server): request-response go through nearest cached server

[Scalability for dummies - Part 3: Cache](https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)